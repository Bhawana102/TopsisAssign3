{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDu69J6ilPLsPUtWGy4qZs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhawana102/TopsisAssign3/blob/main/topsis3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWhOM66PvdyD"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from math import exp\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load Data\n",
        "train_df_ = pd.read_csv(\"/kaggle/input/mydata-csv/train.csv\")\n",
        "train_df = train_df_.head(150)\n",
        "\n",
        "# Select Pretrained Models\n",
        "models = [\n",
        "    \"facebook/bart-large-cnn\",\n",
        "    \"t5-large\",\n",
        "    \"sshleifer/distilbart-cnn-12-6\",\n",
        "    \"google/pegasus-large\",\n",
        "    \"allenai/led-large-16384-arxiv\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "oJ-b_iE0vxqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BLEU Scores list\n",
        "bleu_scores = []\n",
        "\n",
        "# Semantic Coherence (Example implementation)\n",
        "def semantic_coherence(generated_summary, dialogue):\n",
        "    # Your semantic coherence metric calculation logic\n",
        "    summary_tokens = word_tokenize(generated_summary.lower())\n",
        "    dialogue_tokens = word_tokenize(dialogue.lower())\n",
        "\n",
        "    # Calculate the intersection of tokens\n",
        "    common_tokens = set(summary_tokens) & set(dialogue_tokens)\n",
        "\n",
        "    # Calculate semantic coherence score based on the ratio of common tokens to summary length\n",
        "    coherence_score = len(common_tokens) / len(summary_tokens)\n",
        "\n",
        "    return coherence_score\n",
        "\n",
        "# Factual Accuracy (Example implementation)\n",
        "def factual_accuracy(generated_summary, reference_summary):\n",
        "    # Your factual accuracy metric calculation logic\n",
        "    gen_tokens = set(word_tokenize(generated_summary.lower()))\n",
        "    ref_tokens = set(word_tokenize(reference_summary.lower()))\n",
        "\n",
        "    # Calculate the intersection of tokens\n",
        "    common_tokens = gen_tokens & ref_tokens\n",
        "\n",
        "    # Calculate factual accuracy score based on the ratio of common tokens to reference summary length\n",
        "    accuracy_score = len(common_tokens) / len(ref_tokens) if len(ref_tokens) != 0 else 0\n",
        "    return accuracy_score"
      ],
      "metadata": {
        "id": "FepBoPa4vxtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Content Coverage (Example implementation)\n",
        "def content_coverage(generated_summary, dialogue):\n",
        "    # Your content coverage metric calculation logic\n",
        "\n",
        "    summary_tokens = set(word_tokenize(generated_summary.lower()))\n",
        "    dialogue_tokens = set(word_tokenize(dialogue.lower()))\n",
        "\n",
        "    # Calculate the intersection of tokens\n",
        "    common_tokens = summary_tokens & dialogue_tokens\n",
        "\n",
        "    # Calculate the content coverage score based on the ratio of common tokens to dialogue length\n",
        "    coverage_score = len(common_tokens) / len(dialogue_tokens) if len(dialogue_tokens) != 0 else 0\n",
        "    return coverage_score\n"
      ],
      "metadata": {
        "id": "26MxEZSGvxv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize evaluation results DataFrame\n",
        "evaluation_results = pd.DataFrame(columns=[\"Model\", \"BLEU Score\", \"Semantic Coherence\", \"Factual Accuracy\", \"Content Coverage\"])\n",
        "\n",
        "# Initialize empty list to store evaluation results\n",
        "evaluation_results_list = []\n",
        "\n",
        "# Apply Models and Evaluate\n",
        "for model_name in models:\n",
        "    print(f\"Evaluating model: {model_name}\")\n",
        "\n",
        "    # Initialize the summarization pipeline\n",
        "    summarizer = pipeline(\"summarization\", model=model_name, tokenizer=model_name)\n",
        "\n",
        "    # Initialize evaluation metric accumulators\n",
        "    semantic_coherence_scores = []\n",
        "    factual_accuracy_scores = []\n",
        "    content_coverage_scores = []\n",
        "\n",
        "    # Generate summaries\n",
        "    generated_summaries = []\n",
        "    for index, row in tqdm(train_df.iterrows(), total=len(train_df)):  # Use tqdm to show progress\n",
        "        dialogue = row['dialogue']\n",
        "        summary = row['summary']\n",
        "        generated_summary = summarizer(dialogue, max_length=100, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
        "        generated_summaries.append(generated_summary)\n",
        "\n",
        "        # Evaluate Semantic Coherence\n",
        "        coherence_score = semantic_coherence(generated_summary, dialogue)\n",
        "        semantic_coherence_scores.append(coherence_score)\n",
        "\n",
        "        # Evaluate Factual Accuracy\n",
        "        accuracy_score = factual_accuracy(generated_summary, summary)\n",
        "        factual_accuracy_scores.append(accuracy_score)\n",
        "\n",
        "        # Evaluate Content Coverage\n",
        "        coverage_score = content_coverage(generated_summary, dialogue)\n",
        "        content_coverage_scores.append(coverage_score)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Vm1RK0q9v5Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Calculate BLEU Score\n",
        "    reference_summaries = train_df[\"summary\"].tolist()\n",
        "    bleu_score = corpus_bleu([[summary] for summary in reference_summaries], generated_summaries)\n",
        "    bleu_scores.append(bleu_score)\n",
        "\n",
        "    # Append results to the evaluation results list\n",
        "    evaluation_results_list.append({\n",
        "        \"Model\": model_name,\n",
        "        \"BLEU Score\": bleu_score,\n",
        "        \"Semantic Coherence\": sum(semantic_coherence_scores) / len(semantic_coherence_scores),\n",
        "        \"Factual Accuracy\": sum(factual_accuracy_scores) / len(factual_accuracy_scores),\n",
        "        \"Content Coverage\": sum(content_coverage_scores) / len(content_coverage_scores),\n",
        "    })\n",
        "\n",
        "    # Print a separator for clarity\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Concatenate the evaluation results list into a DataFrame\n",
        "evaluation_results = pd.concat([pd.DataFrame(item, index=[0]) for item in evaluation_results_list], ignore_index=True)\n",
        "\n",
        "# Compare Results\n",
        "print(\"BLEU Scores:\", bleu_scores)\n",
        "\n",
        "# Save evaluation results to a CSV file\n",
        "evaluation_results.to_csv(\"evaluation_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "uwZZhXeWv5aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qi6lxqG1v5cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNgIGdzFv5fb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}